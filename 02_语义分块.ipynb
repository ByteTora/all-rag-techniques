{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 语义分块介绍\n",
    "文本分块是检索增强生成（RAG）的重要一步，其中大型文本体裁被划分成有意义的片段，以提高检索精度。\n",
    "与固定长度分块不同，语义分块基于句子之间的内容相似性进行分块。\n",
    "\n",
    "### 关键术语\n",
    "\n",
    "- **百分位数分割法**: 找出所有相似度差值的第 X 百分位数，并在相似度下降幅度超过此值的位置分割块。\n",
    "- **标准差**: 在相似度下降幅度超过低于平均值的 X 个标准差的位置进行分割。\n",
    "- **四分位距(IQR)**: 使用四分位距（Q3-Q1）确定分割点。\n",
    "\n",
    "这个笔记本通过使用 **百分位数法** 来实现语义分块，并在一个示例文本上评估其性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File 从PDF文件中提取文本\n",
    "为了实现RAG，我们首先需要一个文本数据源。在这个例子中，我们使用PyMuPDF库从PDF文件中提取文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file. 提取PDF文件中的文本。\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file. PDF文件的路径。\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF. 从PDF中提取的文本。\n",
    "    \"\"\"\n",
    "    # Open the PDF file 打开PDF文件\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text 初始化一个空字符串来存储提取的文本\n",
    "    \n",
    "    # Iterate through each page in the PDF 迭代每一页PDF\n",
    "    for page in mypdf:\n",
    "        # Extract text from the current page and add spacing 从当前页面提取文本并添加空格\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # Return the extracted text, stripped of leading/trailing whitespace 返回提取的文本，并去掉前后空格\n",
    "    return all_text.strip()\n",
    "\n",
    "# Define the path to the PDF file 定义PDF文件的路径\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file 提取PDF文件中的文本\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Print the first 500 characters of the extracted text 打印提取的文本的前500个字符\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  设置LLM 客户端\n",
    "初始化LLM客户端以生成嵌入和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用硅基流动提供的模型服务，需要先注册账号并申请API key，硅基流动官网：https://www.siliconflow.cn/\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=\"sk-xqmgohpohqgwmrislttlkiodikjzoscyvqdgjmfguvfjodwe\"  # 替换为你的API密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sentence-Level Embeddings 创建句子级嵌入\n",
    "We split text into sentences and generate embeddings. 划分文本为句子并生成嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 257 sentence embeddings.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    Creates an embedding for the given text using OpenAI. 使用bge-m3模型为给定文本创建嵌入。\n",
    "\n",
    "    Args:\n",
    "    text (str): Input text. 输入文本。\n",
    "    model (str): Embedding model name. 嵌入模型名称。\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The embedding vector. 嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(model=model, input=text)\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "\n",
    "# Splitting text into sentences (basic split) 分割文本为句子（基本分割）\n",
    "sentences = extracted_text.split(\". \")\n",
    "\n",
    "# Generate embeddings for each sentence 生成每个句子的嵌入\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} sentence embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity Differences 计算相似度差异\n",
    "We compute cosine similarity between consecutive sentences. 计算连续句子之间的余弦相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors. 计算两个向量之间的余弦相似度。\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector. 第一个向量。\n",
    "    vec2 (np.ndarray): Second vector. 第二个向量。\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity. 余弦相似度。\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute similarity between consecutive sentences 计算连续句子之间的相似度\n",
    "similarities = [cosine_similarity(embeddings[i], embeddings[i + 1]) for i in range(len(embeddings) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Semantic Chunking 实现语义分块\n",
    "We implement three different methods for finding breakpoints. 实现三个不同的方法来找到断点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_breakpoints(similarities, method=\"percentile\", threshold=90):\n",
    "    \"\"\"\n",
    "    Computes chunking breakpoints based on similarity drops. 基于相似度下降计算分块断点。\n",
    "\n",
    "    Args:\n",
    "    similarities (List[float]): List of similarity scores between sentences. 句子之间的相似度分数列表。\n",
    "    method (str): 'percentile', 'standard_deviation', or 'interquartile'. 方法（'percentile'、'standard_deviation'或'interquartile'）。\n",
    "    threshold (float): Threshold value (percentile for 'percentile', std devs for 'standard_deviation'). 阈值（百分位数或标准差）。\n",
    "\n",
    "    Returns:\n",
    "    List[int]: Indices where chunk splits should occur. 分块断点的索引列表。\n",
    "    \"\"\"\n",
    "    # Determine the threshold value based on the selected method 根据所选方法确定阈值\n",
    "    if method == \"percentile\":\n",
    "        # Calculate the Xth percentile of the similarity scores 计算相似度分数的第X百分位数\n",
    "        threshold_value = np.percentile(similarities, threshold)\n",
    "    elif method == \"standard_deviation\":\n",
    "        # Calculate the mean and standard deviation of the similarity scores 计算相似度分数的均值和标准差\n",
    "        mean = np.mean(similarities)\n",
    "        std_dev = np.std(similarities)\n",
    "        # Set the threshold value to mean minus X standard deviations 设定阈值为均值减去X个标准差\n",
    "        threshold_value = mean - (threshold * std_dev)\n",
    "    elif method == \"interquartile\":\n",
    "        # Calculate the first and third quartiles (Q1 and Q3) 计算第一和第三四分位数（Q1和Q3）\n",
    "        q1, q3 = np.percentile(similarities, [25, 75])\n",
    "        # Set the threshold value using the IQR rule for outliers 设定阈值使用IQR规则处理异常值\n",
    "        threshold_value = q1 - 1.5 * (q3 - q1)\n",
    "    else:\n",
    "        # Raise an error if an invalid method is provided 如果提供的无效方法，则引发错误\n",
    "        raise ValueError(\"Invalid method. Choose 'percentile', 'standard_deviation', or 'interquartile'.\")\n",
    "\n",
    "    # Identify indices where similarity drops below the threshold value 识别相似度低于阈值的索引\n",
    "    return [i for i, sim in enumerate(similarities) if sim < threshold_value]\n",
    "\n",
    "# Compute breakpoints using the percentile method with a threshold of 90 计算使用百分位数方法的90%分位数作为阈值计算断点\n",
    "breakpoints = compute_breakpoints(similarities, method=\"percentile\", threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text into Semantic Chunks 将文本分割成语义块\n",
    "We split the text based on computed breakpoints. 分割文本基于计算出的断点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 231\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings.\n"
     ]
    }
   ],
   "source": [
    "def split_into_chunks(sentences, breakpoints):\n",
    "    \"\"\"\n",
    "    Splits sentences into semantic chunks. 将句子分割为语义块。\n",
    "\n",
    "    Args:\n",
    "    sentences (List[str]): List of sentences. 句子列表。\n",
    "    breakpoints (List[int]): Indices where chunking should occur. 分割点的索引列表。\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of text chunks. 文本块列表。\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks 初始化一个空列表来存储文本块\n",
    "    start = 0  # Initialize the start index 初始化开始索引\n",
    "\n",
    "    # Iterate through each breakpoint to create chunks 迭代每个分割点来创建文本块\n",
    "    for bp in breakpoints:\n",
    "        # Append the chunk of sentences from start to the current breakpoint 到当前分割点的句子作为一个文本块添加到列表中\n",
    "        chunks.append(\". \".join(sentences[start:bp + 1]) + \".\")\n",
    "        start = bp + 1  # Update the start index to the next sentence after the breakpoint 更新开始索引到下一个句子的索引\n",
    "\n",
    "    # Append the remaining sentences as the last chunk 将剩余的句子作为最后一个文本块添加到列表中\n",
    "    chunks.append(\". \".join(sentences[start:]))\n",
    "    return chunks  # Return the list of chunks 返回文本块列表\n",
    "\n",
    "# Create chunks using the split_into_chunks function 创建文本块使用split_into_chunks函数\n",
    "text_chunks = split_into_chunks(sentences, breakpoints)\n",
    "\n",
    "# Print the number of chunks created 打印创建的文本块的数量\n",
    "print(f\"Number of semantic chunks: {len(text_chunks)}\")\n",
    "\n",
    "# Print the first chunk to verify the result 打印第一个文本块以验证结果\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Semantic Chunks 将语义块的嵌入创建出来\n",
    "We create embeddings for each chunk for later retrieval. 创建每个块的嵌入，以便后续检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_chunks):\n",
    "    \"\"\"\n",
    "    Creates embeddings for each text chunk. 为每个文本块创建嵌入。\n",
    "\n",
    "    Args:\n",
    "    text_chunks (List[str]): List of text chunks. 文本块列表。\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of embedding vectors. 嵌入向量列表。\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each text chunk using the get_embedding function 生成每个文本块的嵌入使用get_embedding函数\n",
    "    return [get_embedding(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Create chunk embeddings using the create_embeddings function 创建块嵌入使用create_embeddings函数\n",
    "chunk_embeddings = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search 执行语义搜索\n",
    "We implement cosine similarity to retrieve the most relevant chunks. 使用余弦相似度检索最相关的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text chunks for a query. 找到与查询最相关的文本块。\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query. 搜索查询。\n",
    "    text_chunks (List[str]): List of text chunks. 文本块列表。\n",
    "    chunk_embeddings (List[np.ndarray]): List of chunk embeddings. 块嵌入列表。\n",
    "    k (int): Number of top results to return. 返回的最相关的结果数。\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Top-k relevant chunks. 最相关的前k个文本块。\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query 使用get_embedding函数为查询生成嵌入\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding 计算查询嵌入与每个块嵌入的余弦相似度\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks 获取最相似的前k个块的索引\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks 返回最相关的前k个文本块\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "\n",
      "Transparency and Explainability \n",
      "Transparency and explainability are essential for building trust in AI systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy.\n",
      "========================================\n",
      "Context 2:\n",
      "\n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in \n",
      "XAI focuses on developing methods for explaining AI decisions, enhancing trust, and improving \n",
      "accountability.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file 从JSON文件中加载验证数据\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data 提取验证数据中的第一个查询\n",
    "query = data[0]['question']\n",
    "\n",
    "# Get top 2 relevant chunks 获取最相关的前两个文本块\n",
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "# Print the query 打印查询\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Print the top 2 most relevant text chunks 打印最相关的前两个文本块\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i+1}:\\n{chunk}\\n{'='*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成基于检索到的片段的响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义AI助手的系统提示\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant that strictly answers based on the given context. \n",
    "If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"THUDM/GLM-4-9B-0414\"):\n",
    "    \"\"\"\n",
    "    生成基于系统提示和用户消息的AI模型响应。\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): 系统提示用于指导AI的行为。\n",
    "    user_message (str): 用户的消息或查询。\n",
    "    model (str): 要使用的模型。\n",
    "\n",
    "    Returns:\n",
    "    dict: AI模型的响应。\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 创建基于顶部块的用户提示\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# 生成AI响应\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估AI响应\n",
    "比较AI响应与期望答案，并给出分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluation Score: 1\n",
      "\n",
      "#### Detailed Explanation:\n",
      "\n",
      "The AI assistant's response aligns very closely with the true response in both content and structure. Here’s a breakdown of the comparison:\n",
      "\n",
      "1. **Definition of Explainable AI (XAI):**\n",
      "   - **AI Response:** \"Explainable AI (XAI) is a field of research that aims to make AI systems more transparent and understandable.\"\n",
      "   - **True Response:** \"Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions.\"\n",
      "   \n",
      "   The AI response accurately captures the essence of XAI as making AI systems transparent and understandable. While the true response adds the detail of \"providing insights into how they make decisions,\" the core meaning is preserved, and the omission does not significantly detract from the overall accuracy.\n",
      "\n",
      "2. **Importance of XAI:**\n",
      "   - **AI Response:** \"It focuses on developing methods for explaining AI decisions, which enhances trust and improves accountability. XAI is considered important because transparency and explainability are essential for building trust in AI systems, enabling users to assess their fairness and accuracy.\"\n",
      "   - **True Response:** \"It's considered important for building trust, accountability, and ensuring fairness in AI systems.\"\n",
      "   \n",
      "   The AI response comprehensively covers the importance of XAI by mentioning trust, accountability, and fairness. It also explains that transparency and explainability are essential for these reasons, which is consistent with the true response. The true response is more concise but does not add any contradictory or missing information.\n",
      "\n",
      "#### Conclusion:\n",
      "The AI assistant's response is very close to the true response, accurately defining XAI and explaining its importance. The minor omission in the definition does not significantly impact the overall accuracy, and the response effectively conveys the key points. Therefore, the score is 1.\n"
     ]
    }
   ],
   "source": [
    "# 定义评估系统的系统提示\n",
    "evaluate_system_prompt = \"\"\"\n",
    "You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \n",
    "If the AI assistant's response is very close to the true response, assign a score of 1. \n",
    "If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \n",
    "If the response is partially aligned with the true response, assign a score of 0.5. give a detailed explanation of your evaluation.\"\"\"\n",
    "\n",
    "# 创建评估提示，将用户查询、AI响应、真实答案和评估系统提示组合在一起。\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# 生成评估响应，使用评估系统提示和评估提示。\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 打印评估响应\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
