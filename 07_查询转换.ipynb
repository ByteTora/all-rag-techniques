{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "# 查询转换以增强RAG系统\n",
    "\n",
    "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.\n",
    "  \n",
    "  此笔记本实现了三种查询转换技术，以增强RAG系统的检索性能，而无需依赖专业库（如LangChain）。通过修改用户查询，我们可以显著提高检索到的信息的相关性和全面性。\n",
    "\n",
    "\n",
    "## Key Transformation Techniques\n",
    "## 关键转换技术\n",
    "\n",
    "1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.  \n",
    "查询重写：使查询更具体和详细，以提高搜索精度。\n",
    "2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.  \n",
    "步回提示：生成更宽泛的查询以检索有用的上下文信息。\n",
    "3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval.  \n",
    "子查询分解：将复杂查询分解为更简单的组成部分，以实现全面的检索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用硅基流动提供的模型服务，需要先注册账号并申请API key，硅基流动官网：https://www.siliconflow.cn/\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=\"sk-xqmgohpohqgwmrislttlkiodikjzoscyvqdgjmfguvfjodwe\"  # 替换为你的API密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting\n",
    "This technique makes queries more specific and detailed to improve precision in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "        \n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"\n",
    "    You are an AI assistant specialized in improving search queries. \n",
    "    Your task is to rewrite user queries to be more specific, detailed, \n",
    "    and likely to retrieve relevant information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. \n",
    "    Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "This technique generates broader queries to retrieve contextual background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "        \n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"\n",
    "    You are an AI assistant specialized in search strategies. \n",
    "    Your task is to generate broader, more general versions of \n",
    "    specific queries to retrieve relevant background information.\"\"\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query \n",
    "    that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "This technique breaks down complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"\n",
    "    You are an AI assistant specialized in breaking down complex questions. \n",
    "    Your task is to decompose complex queries into simpler sub-questions that, \n",
    "    when answered together, address the original query.\"\"\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. \n",
    "    Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques\n",
    "Let's apply these techniques to an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the impacts of AI on job automation and employment?\n",
      "\n",
      "1. Rewritten Query:\n",
      "Here’s a more specific and detailed version of your query, incorporating relevant terms and concepts to improve search accuracy:  \n",
      "\n",
      "**Rewritten query:**  \n",
      "*\"What are the current and projected impacts of artificial intelligence (AI) and machine learning (ML) on job automation, employment trends, and workforce displacement across different industries? How do factors like skill gaps, reskilling initiatives, and economic policies influence these outcomes? Include case studies or data from sectors like manufacturing, healthcare, and customer service.\"*  \n",
      "\n",
      "### Key improvements:  \n",
      "1. **Specificity** – Clarifies focus on AI/ML (not just general automation).  \n",
      "2. **Timeframe** – Distinguishes between current impacts and future projections.  \n",
      "3. **Scope** – Explicitly mentions industries (manufacturing, healthcare, etc.).  \n",
      "4. **Nuanced factors** – Adds skill gaps, reskilling, and policy influences.  \n",
      "5. **Request for evidence** – Asks for case studies or data to support findings.  \n",
      "\n",
      "This version is more likely to retrieve targeted research, reports, or expert analyses. Let me know if you'd like to emphasize other aspects (e.g., ethical concerns, regional differences).\n",
      "\n",
      "2. Step-back Query:\n",
      "Here are a few broader, more general versions of the original query that could help retrieve useful background information:  \n",
      "\n",
      "1. **\"How does technological advancement affect employment and the labor market?\"**  \n",
      "2. **\"What are the historical trends of automation and its impact on jobs?\"**  \n",
      "3. **\"What are the economic and social effects of automation in industries?\"**  \n",
      "4. **\"How do emerging technologies influence workforce dynamics?\"**  \n",
      "5. **\"What are the key debates around job displacement due to technology?\"**  \n",
      "\n",
      "These step-back queries can help uncover foundational knowledge about automation, technological change, and employment trends before focusing specifically on AI.\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. How does AI contribute to job automation in various industries?\n",
      "   2. What types of jobs are most at risk of being automated by AI?\n",
      "   3. How does AI create new job opportunities or roles in the workforce?\n",
      "   4. What are the potential economic and social consequences of AI-driven job automation?\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
    "\n",
    "# Apply query transformations\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how query transformations integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"Qwen/Qwen3-Embedding-4B\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings as a list of vectors\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # Create embeddings for all chunks at once for efficiency\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add chunks to vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): The model to use for response generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based only on the provided context. \n",
    "    If you cannot find the answer in the context, state that you don't have enough information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the generated response, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, transformed query, context, and response\n",
    "    \"\"\"\n",
    "    # Process the document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Apply query transformation and search\n",
    "    if transformation_type:\n",
    "        # Perform search with transformed query\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # Perform regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # Combine context from search results\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # Generate response based on the query and combined context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the results including original query, transformation type, context, and response\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Compare responses from different query transformation techniques.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from different transformation techniques\n",
    "        reference_answer (str): Reference answer for comparison\n",
    "        model (str): Model for evaluation\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with the reference answer and responses from each technique\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # Define the user prompt with the comparison text\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Define the transformation techniques to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # Run RAG with each transformation technique\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # Get the result for the current transformation type\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # Print the response for the current transformation type\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Compare results if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Response with original query:\n",
      "Based on the provided context, 'Explainable AI' (XAI) refers to techniques and research aimed at making AI systems more transparent and understandable. It focuses on providing insights into how AI systems arrive at their decisions, enabling users to assess their reliability, fairness, and accuracy.\n",
      "\n",
      "Explainable AI is considered important for several reasons:\n",
      "1. **Building Trust**: Transparency and explainability are key to building trust in AI systems (Chapter 20: Building Trust in AI).\n",
      "2. **Accountability**: It helps establish accountability and responsibility for AI systems by making their decision-making processes understandable (Accountability and Responsibility section).\n",
      "3. **Addressing Black Box Issues**: Many AI systems, especially deep learning models, operate as \"black boxes,\" making it difficult to understand their decisions. XAI addresses this challenge (Passage 2).\n",
      "4. **Ethical Behavior**: It supports ethical behavior by allowing users to assess the fairness and accuracy of AI decisions (Passage 1).\n",
      "5. **Research Focus**: XAI is a significant area of AI research, with efforts focused on developing methods to explain AI decisions and improve accountability (Chapter 15: The Future of AI Research).\n",
      "\n",
      "In summary, Explainable AI is crucial for ensuring that AI systems are transparent, accountable, and trustworthy, which are essential for their ethical and responsible deployment.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Rewritten query: Here’s a more specific and detailed version of your query, incorporating relevant terms and concepts to improve search accuracy:  \n",
      "\n",
      "**Rewritten Query:**  \n",
      "*\"What is 'Explainable AI' (XAI), and how does it differ from traditional 'black-box' AI models? What are the key techniques (e.g., SHAP, LIME, decision trees) used to achieve explainability in AI systems? Why is Explainable AI considered critical for ethical AI deployment, regulatory compliance (e.g., GDPR, AI Act), and applications in high-stakes domains like healthcare, finance, and autonomous systems?\"*  \n",
      "\n",
      "This version:  \n",
      "1. Expands the acronym (XAI) for clarity.  \n",
      "2. Contrasts it with \"black-box\" AI to highlight its purpose.  \n",
      "3. Lists specific methods (SHAP, LIME) and domains (healthcare, finance).  \n",
      "4. Ties importance to ethics, regulations, and real-world use cases.  \n",
      "\n",
      "Let me know if you'd like to emphasize other aspects (e.g., technical depth, industry-specific relevance)!\n",
      "Response with rewrite query:\n",
      "Based on the provided context, 'Explainable AI' (XAI) refers to techniques and methods aimed at making AI systems more transparent and understandable. XAI provides insights into how AI models make decisions, allowing users to assess their reliability, fairness, and accuracy. \n",
      "\n",
      "### Importance of Explainable AI:\n",
      "1. **Transparency and Trust**: XAI helps build trust in AI systems by making their decision-making processes understandable to users (Passage 1 and Passage 2).\n",
      "2. **Accountability**: It enhances accountability by clarifying how decisions are made, which is essential for addressing potential harms and ensuring ethical behavior (Passage 1 and Passage 3).\n",
      "3. **Fairness Assessment**: Users can evaluate the fairness of AI decisions, ensuring that the systems operate without bias (Passage 1).\n",
      "4. **Regulatory and Ethical Compliance**: XAI supports compliance with ethical frameworks and regulations by providing clear explanations for AI-driven actions (Passage 2 and Passage 3).\n",
      "5. **Research and Development**: Advances in XAI are a key focus in AI research, aiming to create more interpretable and efficient models (Passage 3).\n",
      "\n",
      "In summary, Explainable AI is crucial for fostering trust, accountability, and ethical use of AI systems across various domains.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Step-back query: Here are a few broader, more general versions of the original query that could help retrieve useful background information:  \n",
      "\n",
      "1. **\"What are the key concepts and principles of artificial intelligence (AI)?\"**  \n",
      "   *(This provides foundational knowledge about AI, which is necessary to understand Explainable AI.)*  \n",
      "\n",
      "2. **\"Why is transparency and interpretability important in machine learning and AI systems?\"**  \n",
      "   *(This shifts focus to the broader concerns that Explainable AI addresses.)*  \n",
      "\n",
      "3. **\"What are the ethical and regulatory challenges in AI development?\"**  \n",
      "   *(This explores the wider context in which Explainable AI becomes relevant.)*  \n",
      "\n",
      "4. **\"How do AI models make decisions, and what are the limitations of black-box models?\"**  \n",
      "   *(This helps explain why Explainable AI is needed by contrasting it with opaque models.)*  \n",
      "\n",
      "5. **\"What are the major trends and concerns in modern AI research?\"**  \n",
      "   *(This gives a high-level overview of where Explainable AI fits in the broader AI landscape.)*  \n",
      "\n",
      "Would you like any of these refined further?\n",
      "Response with step_back query:\n",
      "Based on the provided context, 'Explainable AI' (XAI) refers to techniques and approaches aimed at making AI systems more transparent and understandable. It provides insights into how AI models arrive at their decisions, allowing users to assess the fairness, reliability, and accuracy of those decisions.\n",
      "\n",
      "Explainable AI is considered important for several reasons:\n",
      "1. **Building Trust**: Transparency and explainability are key to building trust in AI systems (Chapter 20: Building Trust in AI).\n",
      "2. **Accountability**: It enhances accountability by making it clearer how decisions are made, which is crucial for addressing potential harms and ensuring ethical behavior (Accountability and Responsibility section).\n",
      "3. **Addressing Black Box Issues**: Many AI systems, especially deep learning models, operate as \"black boxes,\" making their decision-making processes difficult to understand. XAI helps mitigate this issue (Passage 2).\n",
      "4. **Fairness and Reliability**: By making AI decisions understandable, users can better assess their fairness and accuracy (Passage 1).\n",
      "5. **Ethical Frameworks**: XAI supports the establishment of clear guidelines and ethical frameworks for AI development and deployment (Passage 3).\n",
      "\n",
      "In summary, Explainable AI is vital for ensuring that AI systems are transparent, accountable, and trustworthy, which are essential for their ethical and responsible use.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Decomposed into sub-queries:\n",
      "1. What is the definition of 'Explainable AI'?\n",
      "2. How does Explainable AI differ from traditional AI systems?\n",
      "3. What are the key benefits of implementing Explainable AI?\n",
      "4. In what domains or applications is Explainable AI particularly important?\n",
      "Response with decompose query:\n",
      "Based on the provided context, 'Explainable AI' (XAI) refers to techniques and research aimed at making AI systems more transparent and understandable. It focuses on providing insights into how AI systems arrive at their decisions, enabling users to assess their reliability, fairness, and accuracy. \n",
      "\n",
      "Explainable AI is considered important for several reasons:\n",
      "1. **Building Trust**: Transparency and explainability are key to building trust in AI systems, as they help users understand and evaluate the decision-making processes (Chapter 20: Building Trust in AI).\n",
      "2. **Accountability and Responsibility**: XAI enhances accountability by making it clearer how decisions are made, which is essential for addressing potential harms and ensuring ethical behavior (Chapter 20: Accountability and Responsibility).\n",
      "3. **Fairness and Bias Mitigation**: By making AI decisions understandable, XAI helps identify and mitigate biases that may lead to unfair or discriminatory outcomes (Chapter 4: Ethical and Societal Implications of AI).\n",
      "4. **Regulatory and Ethical Compliance**: Explainability aligns with ethical concerns and data protection regulations, ensuring responsible AI deployment (Privacy and Data Protection section).\n",
      "\n",
      "In summary, Explainable AI is crucial for fostering trust, accountability, fairness, and compliance in AI systems.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### **Reference Answer:**\n",
      "- **Content:** Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions. It's considered important for building trust, accountability, and ensuring fairness in AI systems.\n",
      "- **Key Points:** Transparency, trust, accountability, fairness.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Original Query Response**\n",
      "- **Score:** 8/10  \n",
      "- **Strengths:**  \n",
      "  - Covers all key aspects (transparency, trust, accountability, fairness).  \n",
      "  - Provides detailed explanations with references to specific sections (e.g., \"Chapter 20: Building Trust in AI\").  \n",
      "  - Well-structured with a clear summary.  \n",
      "- **Weaknesses:**  \n",
      "  - Slightly repetitive (e.g., \"black box issues\" and \"ethical behavior\" overlap with other points).  \n",
      "  - Could be more concise in summarizing the importance of XAI.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Rewrite Query Response**\n",
      "- **Score:** 9/10  \n",
      "- **Strengths:**  \n",
      "  - Very clear and concise, with a logical flow.  \n",
      "  - Expands on the reference answer by including \"regulatory and ethical compliance\" as an additional key point.  \n",
      "  - Uses bullet points effectively for readability.  \n",
      "- **Weaknesses:**  \n",
      "  - Slightly less detailed in referencing specific sections compared to the original.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Step_back Query Response**\n",
      "- **Score:** 7.5/10  \n",
      "- **Strengths:**  \n",
      "  - Covers all key points (trust, accountability, fairness).  \n",
      "  - Good structure with a clear summary.  \n",
      "- **Weaknesses:**  \n",
      "  - Lacks some depth in explanations (e.g., \"ethical frameworks\" is mentioned but not elaborated).  \n",
      "  - Similar to the original but slightly less polished.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Decompose Query Response**\n",
      "- **Score:** 8.5/10  \n",
      "- **Strengths:**  \n",
      "  - Very detailed, with strong references to specific chapters (e.g., \"Chapter 4: Ethical and Societal Implications of AI\").  \n",
      "  - Includes \"bias mitigation\" as a distinct point, adding depth.  \n",
      "- **Weaknesses:**  \n",
      "  - Slightly less fluid in transitions between points compared to the rewrite.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Ranking of Techniques (Best to Worst):**\n",
      "1. **Rewrite** (9/10) – Most concise, clear, and adds value with regulatory compliance.  \n",
      "2. **Decompose** (8.5/10) – Detailed and well-referenced but slightly less fluid.  \n",
      "3. **Original** (8/10) – Comprehensive but somewhat repetitive.  \n",
      "4. **Step_back** (7.5/10) – Good but lacks depth in some areas.  \n",
      "\n",
      "### **Best Technique: Rewrite**  \n",
      "- **Why?** It balances clarity, conciseness, and completeness while adding a useful point (regulatory compliance) not explicitly in the reference answer. The structure is easy to follow, making it the most effective response.\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
