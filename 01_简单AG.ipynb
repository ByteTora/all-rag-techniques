{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 简单RAG概述\n",
    "\n",
    "检索增强生成（RAG）是一种混合方法，结合了信息检索和生成模型。它通过引入外部知识来增强语言模型的性能，从而提高准确性和事实正确性。\n",
    "\n",
    "在一个简单的RAG设置中，我们按照以下步骤进行：\n",
    "\n",
    "1. **数据引入**：加载并预处理文本数据。\n",
    "2. **分块**：将数据分成较小的块，以提高检索性能。\n",
    "3. **嵌入创建**：使用嵌入模型将文本块转换为数值表示。\n",
    "4. **语义搜索**：基于用户查询检索相关块。\n",
    "5. **响应生成**：使用语言模型生成响应，基于检索到的文本。\n",
    "\n",
    "本笔记本实现了一个简单的RAG方法，评估模型的响应，并探索各种改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取PDF文件中的文本\n",
    "为了实现RAG，我们首先需要一个文本数据源。在这个例子中，我们使用PyMuPDF库从PDF文件中提取文本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本，并打印前`num_chars`个字符。\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    Returns:\n",
    "    str: 从PDF文件中提取出的文本。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 打开PDF文件\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取出的文本\n",
    "\n",
    "    # 迭代PDF中的每一页\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取当前页\n",
    "        text = page.get_text(\"text\")  # 提取文本\n",
    "        all_text += text  # 追加提取出的文本到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取出的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分块抽取文本\n",
    "当我们提取出文本时，我们可以将其分成较小的、重叠的块，以提高检索精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    分割给定的文本为n个字符的段落，并具有重叠。\n",
    "    Args:\n",
    "    text (str): 要分割的文本。\n",
    "    n (int): 每个段落的字符数。\n",
    "    overlap (int): 两个段落之间的重叠字符数。\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: 分割后的文本段落列表。\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化一个空列表来存储分割后的文本段\n",
    "    \n",
    "    # 循环遍历文本，步长为(n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 将文本从索引i到i+n的片段添加到chunks列表中\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 返回文本片段列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  设置LLM 客户端\n",
    "初始化LLM客户端以生成嵌入和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用硅基流动提供的模型服务，需要先注册账号并申请API key，硅基流动官网：https://www.siliconflow.cn/\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.siliconflow.cn/v1/\",\n",
    "    api_key=\"************\"  # 替换为你的API密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取PDF文件中的文本并分块\n",
    "现在，我们加载PDF，提取文本，并将其分块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "#定义PDF文件的路径\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 提取PDF文件中的文本\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 分割文本为1000个字符的片段，每两个片段之间有200个字符的重叠\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# 打印创建的文本片段的数量\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# 打印第一个文本片段\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建文本块的嵌入\n",
    "嵌入将文本转换为数值向量，这使得有效的相似性搜索成为可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-m3\"):\n",
    "    \"\"\"\n",
    "    创建给定文本的嵌入使用指定的llm模型，这里我们选择智源系列。\n",
    "\n",
    "    Args:\n",
    "    text (str): 输入文本，用于创建嵌入。\n",
    "    model (str): 使用bge-m3模型创建嵌入。\n",
    "\n",
    "    Returns:\n",
    "    dict: OpenAI API的响应，其中包含嵌入。\n",
    "    \"\"\"\n",
    "    # 创建输入文本的嵌入使用指定的模型\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response  # 返回包含嵌入的响应\n",
    "\n",
    "# 创建文本片段的嵌入\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语义搜索\n",
    "使用余弦相似度来找到用户查询的最相关文本块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度。\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): 第一个向量。\n",
    "    vec2 (np.ndarray): 第二个向量。\n",
    "\n",
    "    Returns:\n",
    "    float:  两个向量之间的余弦相似度。\n",
    "    \"\"\"\n",
    "    # 计算两个向量的点积，并除以它们范数的乘积\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    执行语义搜索，使用给定的查询和嵌入在文本片段上进行搜索。\n",
    "\n",
    "    Args:\n",
    "    query (str): 查询，用于语义搜索。\n",
    "    text_chunks (List[str]): 文本片段的列表，用于搜索。\n",
    "    embeddings (List[dict]): 嵌入的列表，用于文本片段。\n",
    "    k (int): 返回的最相关的文本片段的数量。默认值为5。\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 返回的最相关的文本片段的列表。\n",
    "    \"\"\"\n",
    "    # 创建查询的嵌入\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # 初始化列表以存储相似性得分\n",
    "\n",
    "    # 计算查询嵌入和每个文本片段嵌入之间的相似性得分\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # 将索引和相似性得分添加到列表中\n",
    "\n",
    "    # 按照相似性得分的降序排序\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # 获取最相似的文本片段的索引\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # 返回最相关的文本片段的列表\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在提取的文本块中运行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy. \n",
      "Privacy and Data Protection \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
      "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
      "and complying with data protection regulations are crucial. \n",
      "Accountability and Responsibility \n",
      "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
      "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
      "developers, deployers, and users of AI systems. \n",
      "Chapter 20: Building Trust in AI \n",
      "Transparency and Explainability \n",
      "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "\n",
      "=====================================\n",
      "Context 2:\n",
      "control, accountability, and the \n",
      "potential for unintended consequences. Establishing clear guidelines and ethical frameworks for \n",
      "AI development and deployment is crucial. \n",
      "Weaponization of AI \n",
      "The potential use of AI in autonomous weapons systems raises significant ethical and security \n",
      "concerns. International discussions and regulations are needed to address the risks associated \n",
      "with AI-powered weapons. \n",
      "Chapter 5: The Future of Artificial Intelligence \n",
      "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
      "across various domains. Key trends and areas of development include: \n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
      "techniques are being developed to provide insights into how AI models make decisions, \n",
      "enhancing trust and accountability. \n",
      "AI at the Edge \n",
      "AI at the edge involves processing data locally on devices, rather than relying on cloud-based \n",
      "servers. This approach reduc\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# 从JSON文件中加载验证数据\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 从验证数据中提取第一个查询\n",
    "query = data[0]['question']\n",
    "\n",
    "# 进行语义搜索以找到查询的前2个最相关的文本块\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# 打印查询\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# 打印前2个最相关的文本块\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成基于检索到的片段的响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义AI助手的系统提示\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant that strictly answers based on the given context. \n",
    "If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"THUDM/GLM-4-9B-0414\"):\n",
    "    \"\"\"\n",
    "    生成基于系统提示和用户消息的AI模型响应。\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): 系统提示用于指导AI的行为。\n",
    "    user_message (str): 用户的消息或查询。\n",
    "    model (str): 要使用的模型。默认值为\"meta-llama/Llama-2-7B-chat-hf\"。\n",
    "\n",
    "    Returns:\n",
    "    dict: AI模型的响应。\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 创建基于顶部块的用户提示\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# 生成AI响应\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估AI响应\n",
    "比较AI响应与期望答案，并给出分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluation Score: 1\n",
      "\n",
      "#### Detailed Explanation:\n",
      "\n",
      "The AI assistant's response is very close to the true response. Here’s a breakdown of the alignment:\n",
      "\n",
      "1. **Definition of Explainable AI (XAI):**\n",
      "   - **AI Response:** \"Explainable AI (XAI) is a set of techniques aimed at making AI decisions more understandable, enabling users to assess their fairness and accuracy.\"\n",
      "   - **True Response:** \"Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions.\"\n",
      "   \n",
      "   The AI response accurately captures the essence of XAI by describing it as a set of techniques that enhance understandability and allow users to assess fairness and accuracy. The true response also emphasizes transparency and providing insights into decision-making processes, which aligns well with the AI response.\n",
      "\n",
      "2. **Importance of XAI:**\n",
      "   - **AI Response:** \"It is considered important because it enhances transparency and explainability, which are key to building trust in AI. By providing insights into how AI models make decisions, XAI techniques help users evaluate the reliability and fairness of AI systems, thereby fostering accountability and responsible use of AI.\"\n",
      "   - **True Response:** \"It's considered important for building trust, accountability, and ensuring fairness in AI systems.\"\n",
      "   \n",
      "   The AI response comprehensively covers the importance of XAI by highlighting transparency, explainability, trust, reliability, fairness, accountability, and responsible use. The true response succinctly captures these key points, and the AI response provides a more detailed elaboration that is still very close to the true response.\n",
      "\n",
      "#### Conclusion:\n",
      "The AI assistant's response is highly accurate and aligns closely with the true response in both the definition and the importance of Explainable AI. Therefore, the score is 1.\n"
     ]
    }
   ],
   "source": [
    "# 定义评估系统的系统提示\n",
    "evaluate_system_prompt = \"\"\"\n",
    "You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \n",
    "If the AI assistant's response is very close to the true response, assign a score of 1. \n",
    "If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. \n",
    "If the response is partially aligned with the true response, assign a score of 0.5. give a detailed explanation of your evaluation.\"\"\"\n",
    "\n",
    "# 创建评估提示，将用户查询、AI响应、真实答案和评估系统提示组合在一起。\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# 生成评估响应，使用评估系统提示和评估提示。\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 打印评估响应\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
